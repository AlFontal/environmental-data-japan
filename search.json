[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Japan's Environmental Data",
    "section": "",
    "text": "Repository collecting and organizing environmental data sources for Japan.\nHere, we have downloaded and processed the data available through the site made available by the Japanese National Institute for Environmental Studies (NIES) (https://tenbou.nies.go.jp/download/).\nMore specifically, we have downloaded the Air Pollution Monitoring data for a total of 2039 stations located around the whole country (represented as red dots in the map below):\n\n\n\nInformation for the exact location of each station (latitude, longitude and altitude) and the environmental variables available can be found in the table in /data/air_pollution/stations/doc/stations_info.csv\n\n\n\nSince the repository includes data for over 2000 stations, the measured variables in each of them are rather heterogeneous. The data is provided in hourly format by the Japanese NIES, and the processed files available in this repository provide the data with a daily frequency, so an aggregation method has been used. The following table summarizes the selected variables, their units, and the aggregation method used to summarize them.\n\n\n\n\n\n\n\n\n\nVariable\nName\nUnits\nAggregation Method\n\n\n\n\nSO2\nSulfur Dioxide\nppm\nDaily Mean\n\n\nNO\nNitric Oxide\nppm\nDaily Mean\n\n\nNO2\nNitrogen Dioxide\nppm\nDaily Mean\n\n\nNOX\nNitrogen Oxides\nppm\nDaily Mean\n\n\nCO\nCarbon Monoxide\nppm\nDaily Mean\n\n\nOX\nPhotochemical Oxidants\nppm\nDaily Mean\n\n\nNMHC\nNon-methane Hydrocarbons\nppmC\nDaily Mean\n\n\nCH4\nMethane\nppmC\nDaily Mean\n\n\nTHC\nTotal Hydrocarbons\nppmC\nDaily Mean\n\n\nSPM\nSuspended Particulate Matter (<10µm)\nmg/m³\nDaily Mean\n\n\nPM2.5\nParticulate matter (<2.5µm)\nµg/m³\nDaily Mean\n\n\nTEMP\nTemperature\n°C\nDaily Min, Mean and Max\n\n\nHUM\nRelative Humidity\n%\nDaily Min, Mean and Max\n\n\nRAIN\nTotal Precipitation\nmm\nDaily Sum\n\n\n\n\n\n\n\n\nDaily data for each station and its measured variables from 2011-01-01 to 2018-12-31 can be accessed in the data/air_pollution/stations/clean folder, classified in subfolders according to the prefecture where they are located.\n\n\n\nAveraged daily data for all stations within each municipality in Japan during the same period can be found in the data/air_pollution/municipalities folder, with the first 2 digits of each CSV file indicating the prefecture code and the last 3 digits indicating the municipal code.\n\n\n\nEstimates of the daily data for each prefecture have been estimated by averaging over the municipal averages in each prefecture.\nAt this point, three different methods have been used to compute the averages, weighing the municipalities differently according to diverse criteria:\n\nArea-based weighted averages: These averages are completely population-naïve and try to better estimate the overall values for each variable across the whole area of the prefecture, wheighing high and low population density areas exactly the same. The time-series for these can be found in data/air_pollution/prefectures/area_weighted.\nPopulation-weighted averages: These averages are computed trying to estimate the values of each variable that represent the most accurate exposure of the population of each prefecture to each pollutant/meteorological variable. For this purpose, the values for each variable have been averaged weighing every municipal term according to their population share in the prefecture. The time-series for these can be found in data/air_pollution/prefectures/population_weighted.\nInfant population-weighted averages: Following the same principal as the previous averages, these include the daily prefecture-level estimates of each measured variable based on the share of children under 5 years old in each municipal term. The time-series for these can be found in data/air_pollution/prefectures/under_5_weighted.\n\n\n\nThe following figure shows the ratio of population covered by the stations in each prefecture (as a ratio of the people living in municipalities with at least one monitoring station):"
  },
  {
    "objectID": "prefecture_weighted_averages.html",
    "href": "prefecture_weighted_averages.html",
    "title": "Japanese Air Pollution Data",
    "section": "",
    "text": "Show code imports and presets\n\n\n\n\n\n\nimport os\nimport requests\n\nimport numpy as np\nimport pandas as pd\nimport plotnine as p9\nimport geopandas as gpd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom IPython.display import Image\nfrom collections import defaultdict\nfrom shapely.geometry import box, Point\nfrom matplotlib_inline.backend_inline import set_matplotlib_formats\nfrom mizani.formatters import date_format, percent_format, custom_format\n\n\n\n\n\ntqdm.pandas()\nset_matplotlib_formats('retina')\np9.options.set_option('dpi', 600)\np9.options.set_option('figure_size', (4, 3))\np9.options.set_option('base_family', 'Georgia')\np9.theme_set(p9.theme_bw() + p9.theme(axis_text=p9.element_text(size=7),\n                                      axis_title=p9.element_text(size=9),\n                                      title=p9.element_text(size=11)))"
  },
  {
    "objectID": "prefecture_weighted_averages.html#data-loading",
    "href": "prefecture_weighted_averages.html#data-loading",
    "title": "Japanese Air Pollution Data",
    "section": "Data Loading",
    "text": "Data Loading\nWith daily data of the monitoring stations already collected from the official site, cleaned, and pre-processed to daily averages stored in data/air_pollution/stations/clean, we are now going to collect two different sources of information that should enable us to compute population weighted averages of the station data per prefecture. The files will be the following:\n\nPopulation density per municipality in Japan\nShapefiles of the said municipalities so as to be able to match the location of the monitoring station to a certain number of population\n\n\nFetching the shapes\nWhile several sources of shapefiles ara available online, I could not find any that included the same municipal codes that would enable a match with the population data obtained from the official Japanese statistics site.\nThis site, however, also provides a very precise collection of shapefiles for each of the 47 prefectures, with areas smaller than the municipal level (the one we are interested in), but with municipal-level identifiers, so we can fuse the shapes to obtain exactly the shapes for all municipalities in Japan.\nLet’s do exactly that:\n\ndef get_download_url(code):\n    url = 'https://www.e-stat.go.jp/gis/statmap-search/data?dlserveyId=A002005212015' \\\n         f'&code={code}&coordSys=1&format=shape&downloadType=5&datum=2000'\n    return url\n\n\nshapes_path = '../data/shapefiles/administrative_boundaries'\nif not os.path.exists(f'{shapes_path}/municipality_shapes.shp'):\n    municipality_shapes = []\n    for i in tqdm(range(1, 48), total=47):\n        municipality_shapes.append(\n            gpd.read_file(get_download_url(str(i).zfill(2))).dissolve(by='CITY')\n        )\n\n    municipality_shapes = pd.concat(municipality_shapes).reset_index()[['PREF', 'CITY', 'geometry']]\n    prefecture_shapes = municipality_shapes.dissolve(by='PREF').reset_index()[['PREF', 'geometry']]\n    municipality_shapes.to_file(f'{shapes_path}/municipality_shapes.shp')\n    prefecture_shapes.to_file(f'{shapes_path}/prefecture_shapes.shp')\nelse:\n    municipality_shapes = gpd.read_file(f'{shapes_path}/municipality_shapes.shp')\n    prefecture_shapes = gpd.read_file(f'{shapes_path}/prefecture_shapes.shp')\n\nWe now have the equivalent shapefiles grouped at the municipal and the prefectural level, leading to a map of Japan like the following:\n\n(p9.ggplot(municipality_shapes) \n    + p9.geom_map(alpha=.2, size=.05, color='gray')\n    + p9.geom_map(data=prefecture_shapes, size=.15, alpha=0)\n    + p9.scale_y_continuous(labels=custom_format('{:0g}°N'), limits=(30, 45))\n    + p9.scale_x_continuous(labels=custom_format('{:0g}°E'), limits=(129, 145.5))\n    + p9.theme(figure_size=(2.5, 2.5),\n               axis_text=p9.element_text(size=5),\n               panel_grid=p9.element_blank())\n)\n\n\n\n\n<ggplot: (693277961)>\n\n\n\n\nFetching municipalities populations\nThe same Statistics portal that we used to fetch the shapefiles provides an API to fetch different variables recorded by the government. It is necessary to register in the system to obtain an identifier that accompanies the requests to the API in order to be granted access. The documentation to use the API can be found here (it’s all in Japanese but it’s manageable with Google translate).\nWe’ll start by defining the API URL preceding the parameters specifying the version of the API we will be using (3.0) and the output we want (json):\n\nAPI_URL = 'http://api.e-stat.go.jp/rest/3.0/app/json/getStatsData'\n\nI have already checked the variables I am interested in, although there are plenty more to choose. We will be fetching the following codes:\n\njp_api_codes = {'A1101': 'Total population (Both sexes)',\n                'A110101': 'Total population (Male)',\n                'A110102': 'Total population (Female)',\n                'A1301': 'Total population (under 15)',\n                'A130101': 'Population (under 15, Male)',\n                'A130102': 'Population (under 15, Female)',\n                'A1302': 'Total population (15-64)',\n                'A130201': 'Population (15-64, Male)',\n                'A130202': 'Population (15-64, Female)',\n                'A1303': 'Total population (65 and over)',\n                'A130301': 'Population (65 and over, Male)',\n                'A130302': 'Population (65 and over, Female)',\n                'A1405': 'Population (0-5, Total)',\n                'A140501': 'Population (0-5, Male)',\n                'A140502': 'Population (0-5, Female)',\n                'A1416': 'Population (60 and over, Total)',\n                'A141601': 'Population (60 and over, Male)',\n                'A141602': 'Population (60 and over, Female)',\n                'A1417': 'Population (70 and over, Total)',\n                'A141701': 'Population (70 and over, Male)',\n                'A141702': 'Population (70 and over, Female)',\n                'A1420': 'Population (85 and over, Total)',\n                'A142001': 'Population (85 and over, Male)',\n                'A142002': 'Population (85 and over, Female)'\n               }\n\nI have saved my personal appID in a non-shared file in the root folder as .appID. Let’s read it so as to include it in the request URL:\n\nwith open('../.appID', 'r') as fh:\n    API_ID = fh.read()\n\nWe need to URL encode all the variables in the requests, which should be done by joining them with the %2C characters:\n\nvariables_string = '%2C'.join(list(jp_api_codes.keys()))\n\nThe full request will be the following:\n\nurl = (f'{API_URL}'                   # Main API URL\n       f'?cdCat01={variables_string}' # Specify the variables selected\n       f'&appId={API_ID}'             # Add the API ID to be allowed access \n        '&lang=E'                     # Specify the language to be English\n        '&statsDataId=0000020201'     # Specify the dataset to be used\n        '&cdTimeFrom=2015')           # Request data from the last census (2015)\n\n\nr = requests.get(url)\n\n\nfull_population_df = (pd.DataFrame(r.json()\n                      ['GET_STATS_DATA']['STATISTICAL_DATA']['DATA_INF']['VALUE'])\n                      .loc[lambda dd: dd['@time'].str.startswith('2015')]\n                      .drop(columns=['@tab', '@time', '@unit'])\n                      .rename(columns={'@area': 'area_code'})\n                      .pivot('area_code', '@cat01', '$')\n                      .rename(columns=jp_api_codes)\n)\n\nI will keep an already processed version of this dataset in the repository as an easily accessible CSV table to ease future usage of the files.\n\nfull_population_df.to_csv('../data/population/municipal_population_stats.csv')\n\n\nfull_population_df = pd.read_csv('../data/population/municipal_population_stats.csv')"
  },
  {
    "objectID": "prefecture_weighted_averages.html#matching-stations-municipalities-and-populations",
    "href": "prefecture_weighted_averages.html#matching-stations-municipalities-and-populations",
    "title": "Japanese Air Pollution Data",
    "section": "Matching stations, municipalities, and populations",
    "text": "Matching stations, municipalities, and populations\nSince my main interest lies on relating the environmental variables to Kawasaki Disease incidence in Japan, I am going to use the municipal population of children under 5 years old to weight the importance of each municipal term in the prefecture averages:\n\njp_pops_kids = (full_population_df\n    .reset_index()\n    .rename(columns={'area_code': 'code'})\n    .assign(code=lambda dd: dd.code.astype(str).str.zfill(5))\n    .assign(population=lambda dd: dd['Population (0-5, Total)'].astype(int))\n    [['code', 'population']]\n)\n\n\njp_pops_full = (full_population_df\n    .reset_index()\n    .rename(columns={'area_code': 'code'})\n    .assign(code=lambda dd: dd.code.astype(str).str.zfill(5))\n    .assign(population=lambda dd: dd['Total population (Both sexes)'].astype(int))\n    [['code', 'population']]\n)\n\n\njp_munis = (municipality_shapes\n            .reset_index()\n            .assign(code=lambda dd: dd.PREF + dd.CITY.astype(str))\n            [['PREF', 'CITY', 'code', 'geometry']]\n            )\n\nWe can now load the monitoring stations information, and use their coordinates to assign a municipal boundary to each of them. This way, we will be able to average a value for each municipality using the values of the monitoring stations within its boundaries.\n\ninfo_dir = '../data/air_pollution/stations/doc/stations_info.csv'\nmonitoring_stations = (pd.read_csv(info_dir)\n        .assign(geometry=lambda dd: [Point(np.array([x, y])) \n                 for x, y in zip(dd.longitude, dd.latitude)])\n        .pipe(lambda dd: gpd.GeoDataFrame(dd, geometry='geometry', crs='EPSG:4612'))\n)\n\n\nstation_munis = (gpd.sjoin(monitoring_stations, jp_munis, predicate='within')\n                 .merge(jp_munis\n                        [['code', 'geometry']]\n                        .rename(columns={'geometry': 'm_polygon'}))\n                )\n\n\nstations_per_city = station_munis.groupby(['PREF', 'CITY']).station_code.unique()\n\n\nprefectures_map = (monitoring_stations\n                    [['prefecture', 'prefecture_code']]\n                    .drop_duplicates()\n                    .set_index('prefecture_code')\n                    ['prefecture']\n                    .to_dict()\n)"
  },
  {
    "objectID": "prefecture_weighted_averages.html#generating-the-daily-averaged-municipal-estimates",
    "href": "prefecture_weighted_averages.html#generating-the-daily-averaged-municipal-estimates",
    "title": "Japanese Air Pollution Data",
    "section": "Generating the daily averaged municipal estimates",
    "text": "Generating the daily averaged municipal estimates\n\nmunicipal_pollution_dfs = []\nfor (pref, city), stations in tqdm(stations_per_city.items(), total=len(stations_per_city)):\n    municipal_stations = []\n    pref_name = prefectures_map[int(pref)]\n    for station in stations:\n        municipal_stations.append(\n        pd.read_csv(f'../data/air_pollution/stations/clean/{pref}_{pref_name}/{station}.csv')\n                    .assign(date=lambda dd: pd.to_datetime(dd.date))\n        )\n    \n    municipal_stations = (pd.concat(municipal_stations)\n                          .set_index('date')\n                          .resample('D')\n                          .mean())\n    municipal_pollution_dfs.append(municipal_stations.assign(pref=pref, city=city))\n    municipal_stations.round(3).to_csv(f'../data/air_pollution/municipalities/{pref}{city}.csv')\nmunicipal_pollution_dfs = pd.concat(municipal_pollution_dfs)"
  },
  {
    "objectID": "prefecture_weighted_averages.html#generating-the-daily-averaged-prefectural-estimates",
    "href": "prefecture_weighted_averages.html#generating-the-daily-averaged-prefectural-estimates",
    "title": "Japanese Air Pollution Data",
    "section": "Generating the daily averaged prefectural estimates",
    "text": "Generating the daily averaged prefectural estimates\nNow, by using the municipal values and weighting them by different factors, we will be able to obtain prefecture-level estimates:\n\nlong_pollution_df = (municipal_pollution_dfs\n                     .reset_index()\n                     .melt(['date', 'pref', 'city'])\n                     .dropna()\n                     .eval('code = pref + city')\n)\n\n\nArea-weighted averages\nThe first prefectural estimates we will estimate will be population agnostic, that is, each municipality will have a weight proportional to its area:\n\njp_areas = (jp_munis\n            .eval('area = geometry.to_crs(\"+proj=cea\").area / 10**6')\n            [['code', 'area']]\n)\n\n\nfor prefecture, df in long_pollution_df.groupby('pref'):\n    pref_df = (df.groupby(['date', 'variable'])\n                 .progress_apply(lambda df: df\n                                        .merge(jp_areas)\n                                        .eval('weight = area / area.sum()')\n                                        .eval('value = value * weight')\n                                        .value.sum())\n                 .rename('value')\n                 .reset_index()\n                 .pivot(index='date', columns='variable', values='value')\n)\n    pref_df.round(3).to_csv(f'../data/air_pollution/prefectures/area_weighted/{prefecture}.csv')\n\n\n\nWeighting by population of children under 5 years old\nIn this case we will weigh its municipality proportional to the relative amount of children under 5 years old living in it:\n\nfor prefecture, df in long_pollution_df.groupby('pref'):\n    pref_df = (df.groupby(['date', 'variable'])\n                 .progress_apply(lambda df: df\n                                        .merge(jp_pops_kids)\n                                        .eval('weight = population / population.sum()')\n                                        .eval('value = value * weight')\n                                        .value.sum())\n                 .rename('value')\n                 .reset_index()\n                 .pivot(index='date', columns='variable', values='value')\n)\n    pref_df.round(3).to_csv(f'../data/air_pollution/prefectures/under_5_weighted/{prefecture}.csv')\n\n\n\nWeighting by full population density\nIn the last case, we will weigh each municipality proportional to the total population living in each term.\n\nfor prefecture, df in long_pollution_df.groupby('pref'):\n    pref_df = (df.groupby(['date', 'variable'])\n                 .progress_apply(lambda df: df\n                                        .merge(jp_pops_full)\n                                        .eval('weight = population / population.sum()')\n                                        .eval('value = value * weight')\n                                        .value.sum())\n                 .rename('value')\n                 .reset_index()\n                 .pivot(index='date', columns='variable', values='value')\n)\n    pref_df.round(3).to_csv(f'../data/air_pollution/prefectures/population_weighted/{prefecture}.csv')"
  },
  {
    "objectID": "prefecture_weighted_averages.html#visualizing-the-data",
    "href": "prefecture_weighted_averages.html#visualizing-the-data",
    "title": "Japanese Air Pollution Data",
    "section": "Visualizing the data",
    "text": "Visualizing the data\n\nNumber of stations per variable per prefecture\nWhile there are many stations, not all variables are measured with the same consistency. The following table shows how many stations measure each variable in each prefecture:\n\nvariable_map = {'NOX': 'NO$_{x}$',\n                'PM25': 'PM$_{2.5}$',\n                'SO2': 'SO$_2$',\n                'CO2': 'CO$_2$',\n                'SPM': 'PM$_{10}$'}\n\n\n(monitoring_stations\n .drop(columns=['station_code', 'latitude', 'longitude', 'altitude'])\n .groupby(['prefecture', 'prefecture_code'])\n .sum(numeric_only=True)\n .astype(int)\n .sort_values('prefecture_code')\n .apply(lambda x: x / monitoring_stations.groupby('prefecture_code').station_code.nunique())\n .applymap(lambda x: round(x * 100, 2))\n .reset_index()\n .melt(['prefecture', 'prefecture_code'])\n .query('not variable.isin([\"NO\", \"NO2\", \"CH4\", \"NMHC\"])')\n .replace(variable_map)\n .merge(monitoring_stations\n        .groupby('prefecture_code')\n        .station_code\n        .nunique()\n        .rename('n_stations')\n        .reset_index())\n .assign(pref_name=lambda dd: dd.prefecture.str.split('-').str[0] + \n                                ' (' + dd.n_stations.astype(str) + ')')\n .pipe(lambda dd: p9.ggplot(dd) \n                + p9.aes(x='reorder(variable, value, ascending=True)',\n                         y='reorder(pref_name, value, ascending=False)',\n                         fill='value') \n                + p9.geom_tile(color='black')\n                + p9.geom_text(p9.aes(label='value.round(0).astype(int)',\n                                color='value > 70'), size=2.5)      \n                + p9.theme_minimal()\n                + p9.scale_color_manual(values=['black', 'white'])\n                + p9.scale_fill_continuous('Oranges')\n                + p9.guides(fill=False, color=False)\n                + p9.labs(x='', y='', title='Percentage of stations collecting data')\n                + p9.coord_flip()\n                + p9.theme(figure_size=(4, 1.25),\n                           title=p9.element_text(size=8),\n                           axis_text_x=p9.element_text(angle=90, size=3.5, margin={'t': -10}),\n                           axis_text_y=p9.element_text(size=4, ha='right', margin={'r': -10}),\n                           panel_grid=p9.element_blank())\n)\n)\n\n\n\n\n<ggplot: (774713663)>\n\n\nOnly NOx, SPM (or PM10), SO2 and PM2.5 are variables measured by over 50% of all stations, the rest are present in smaller numbers, which might limit the accuracy of the prefecture-level estimates:\n\n(monitoring_stations\n .drop(columns=['station_code', 'latitude', 'longitude', 'altitude'])\n .groupby(['prefecture', 'prefecture_code'])\n .sum(numeric_only=True)\n .astype(int)\n .sum()\n .pipe(lambda x: x / monitoring_stations.station_code.nunique())\n .rename('ratio')\n .reset_index()\n .query('not index.isin([\"NO\", \"NO2\", \"CH4\", \"NMHC\"])')\n .replace(variable_map)\n .pipe(lambda dd: p9.ggplot(dd) \n                + p9.aes('reorder(index, ratio)', 'ratio') \n                + p9.geom_col() \n                + p9.coord_flip()\n                + p9.labs(x='', y='')\n                + p9.scale_y_continuous(labels=percent_format()) \n                + p9.theme(figure_size=(2, 1.5),\n                           axis_text=p9.element_text(size=4),\n                           panel_grid=p9.element_blank())\n )\n)\n\n\n\n\n<ggplot: (775098065)>\n\n\n\n\nEstimation of population coverage per prefecture\nWe will apply a quite naïve (and probably conservative) method to estimate the percentage of population ‘covered’ by monitoring stations: the % of population in a municipality with at least a station. The population living in municipalities without any station will be considered as uncovered by this metric (which might be a tad unfair as municipal terms can often be really small).\n\njp_munis = jp_munis.assign(has_station=lambda dd: dd.code.isin(station_munis.code))\n\n\npop_coverage = (jp_munis\n                .merge(jp_pops_full)\n                .groupby('PREF')\n                .apply(lambda dd: dd.query('has_station').population.sum() / dd.population.sum())\n                .rename('pop_coverage')\n                .reset_index()\n                .set_index('PREF')\n)\n\nThe prefecture of Tokyo has many small islands (barely inhabited) which distort completely the shape of the region. Let’s get ride of those for the plots:\n\njp_munis = pd.concat([jp_munis.query(\"PREF!='13'\"),\n                      jp_munis.query(\"PREF=='13'\")\n                              .query('geometry.bounds.miny > 35')])\n\n\nPopulation map\n\n(p9.ggplot(jp_munis.merge(jp_pops_full))\n+ p9.geom_map(p9.aes(fill='population'), size=.01) \n+ p9.geom_map(data=prefecture_shapes, size=.05, alpha=0)\n+ p9.scale_fill_continuous('Oranges', breaks=[0, 250_000, 500_000, 750_000], labels=['0', '250k', '500k', '750k'])\n+ p9.scale_y_continuous(labels=custom_format('{:0g}°N'), limits=(30, 45))\n+ p9.scale_x_continuous(labels=custom_format('{:0g}°E'), limits=(129, 145.5))\n+ p9.theme_void()\n+ p9.labs(fill='Population (n)')\n+ p9.theme(figure_size=(3, 3),\n           legend_key_size=5,\n           legend_position=(.35, .75),\n           legend_direction='vertical',\n           legend_title=p9.element_text(size=6),\n           legend_text=p9.element_text(size=4, va='baseline'),)\n)\n\n\n\n\n<ggplot: (774708726)>\n\n\n\n\nStations map\n\nf = (p9.ggplot(jp_munis) \n    + p9.geom_map(size=.01, fill='white') \n    + p9.geom_map(data=prefecture_shapes, size=.05, alpha=0)\n    + p9.geom_point(p9.aes(x='longitude', y='latitude'), data=monitoring_stations, size=.3, stroke=0, alpha=.7, color='red')\n    + p9.scale_y_continuous(limits=(30, 45))\n    + p9.scale_x_continuous(limits=(129, 145.5))\n    + p9.guides(fill=False)\n    + p9.theme_void()\n    + p9.theme(figure_size=(3, 3),\n               dpi=600)\n).draw()\n\nf.savefig('../data/air_pollution/stations/doc/stations_map.png')\n\n\n\n\n\n\nPopulation coverage map\n\nstats_df = defaultdict(list)\nfor i in range(1, 48):\n    shape = jp_munis.query(f'PREF==\"{str(i).zfill(2)}\"')\n    stations = monitoring_stations.query(f'prefecture_code=={i}')\n    stats_df['prefecture'].append(prefectures_map[i].split('-')[0])\n    stats_df['prefecture_code'].append(i)\n    stats_df['n_stations'].append(len(stations))\n    stats_df['coverage'].append((pop_coverage.loc[str(i).zfill(2)] * 100).round(2).values[0])\n    bounds = shape.bounds.agg(['min', 'max']).values\n    stats_df['ar'].append((bounds[1, 3] - bounds[0, 1]) / (bounds[1, 2] - bounds[0, 0]))\nstats_df = pd.DataFrame(stats_df)\n\nTo actually visualize the prefecture by prefecture population coverage, I generated a figure for each prefecture which was later on merged together in a single figure with Inkscape. Notice how in most cases, stations and population go together, as most of the stationless municipalities are usually those which are also lowly populated.\n\nfor i in tqdm(stats_df.sort_values('ar').prefecture_code):\n    shape = jp_munis.query(f'PREF==\"{str(i).zfill(2)}\"')\n    stations = monitoring_stations.query(f'prefecture_code=={i}')\n    n_stations = len(stations)\n    prefecture_name = prefectures_map[i].split('-')[0]\n    coverage = (pop_coverage.loc[str(i).zfill(2)] * 100).round(2).values[0]\n    bounds = shape.bounds.agg(['min', 'max']).values\n    ar = (bounds[1, 3] - bounds[0, 1]) / (bounds[1, 2] - bounds[0, 0])\n    f = (p9.ggplot(shape) \n                + p9.geom_map(p9.aes(fill='population'), size=.02) \n                + p9.geom_point(p9.aes(x='longitude', y='latitude'),\n                                data=stations, size=.3, stroke=0)\n                + p9.labs(x='', y='', fill='')\n                + p9.guides(fill=False)\n                + p9.scale_fill_continuous('Oranges')\n                + p9.ggtitle(f'{prefecture_name} ({n_stations}, {coverage}%)')\n                + p9.theme_void()\n                + p9.theme(aspect_ratio=ar,\n                           figure_size=(1, 1),\n                           dpi=1000,\n                           title=p9.element_text(size=4),)\n\n ).save(f'../data/doc/images/{str(i).zfill(2)}_{prefecture_name}.pdf',\n        bbox_inches='tight', verbose=False)\n\n\nImage('../data/doc/images/prefectures_vertical.png')"
  }
]